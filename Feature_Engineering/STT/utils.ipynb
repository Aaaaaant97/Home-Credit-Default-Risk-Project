{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c456f89c-8a9b-459b-a116-f7609913396b",
   "metadata": {},
   "source": [
    "These are some plotting and model training tools\n",
    "\n",
    "(1)missing_values_table:    Missing value statistics\n",
    "\n",
    "(2)plot_stats:              Plot feature distribution and default rate\n",
    "\n",
    "(3)plot_distribution:       Plot probability density map\n",
    "\n",
    "(4)plot_corr:               Plot Correlation coefficient heatmap\n",
    "\n",
    "(5)plot_bin：               Plot Bin division\n",
    "\n",
    "(6)model:                   Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe92ac5-3ee1-44d6-954e-b37155029c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # for making plots with seaborn\n",
    "\n",
    "# Missing value statistics\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        print(f\"missing ratio > 50% : {len(mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] > 50])}\")\n",
    "        print(f\"missing ratio > 20% : {len(mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] > 20])}\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "##Drawing function, through which the distribution of data can be visually observed\n",
    "def plot_stats(app_train, feature,label_rotation=False,horizontal_layout=True):\n",
    "    temp = app_train[feature].value_counts()\n",
    "    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n",
    "\n",
    "    # Calculate the percentage of defaulting users (TRAGRT=1)\n",
    "    cat_perc = app_train[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n",
    "    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n",
    "    # print(cat_perc)\n",
    "\n",
    "    categories = temp.index.to_list()\n",
    "    categories_count = [str(x) for x in categories]\n",
    "    values_count = temp.values\n",
    "\n",
    "    categories_rate = [str(int(x[0])) for x in cat_perc.values]\n",
    "    values_rate = [x[1] for x in cat_perc.values]\n",
    "    # print(\"categories_count：\", categories_count)\n",
    "    # print(\"categories_rate:\", categories_rate)\n",
    "    # print(\"values_count:\", values_count)\n",
    "    # print(\"values_rate\", values_rate)\n",
    "\n",
    "    colors = sns.color_palette(\"husl\", n_colors=50)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(2,1, 1)\n",
    "    plt.bar(\n",
    "        x=categories_count,  # x\n",
    "        height=values_count, \n",
    "        color=colors[0:len(categories_count)],  # color\n",
    "        width=0.4 \n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel(\"Number of Features\", fontsize=10)\n",
    "\n",
    "    plt.subplot(2,1, 2)\n",
    "    plt.bar(\n",
    "        x=categories_rate, \n",
    "        height=values_rate,\n",
    "        color=colors[0:len(categories_rate)],\n",
    "        width=0.4 \n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel(\"Percent of Default rate [%]\", fontsize=10) \n",
    "\n",
    "    # Add overall title, located at the top\n",
    "    plt.suptitle(f'Distribution of {feature} and Default rate by TARGET', fontsize=13, y=0.98)\n",
    "\n",
    "    # Adjust layout and display\n",
    "    fig.align_ylabels()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(feature + 'user_profile.png')\n",
    "    \n",
    "\n",
    "def plot_distribution(app_train, feature_list):\n",
    "    # iterate through the new features\n",
    "    plt.figure()\n",
    "    for i, feature in enumerate(feature_list):\n",
    "        \n",
    "        # create a new subplot for each source\n",
    "        # plt.subplot(1, 3, i + 1)\n",
    "        # plot repaid loans\n",
    "        sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, feature], label='TARGET == 0')\n",
    "        # plot loans that were not repaid\n",
    "        sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, feature], label='TARGET == 1')\n",
    "        \n",
    "        # Label the plots\n",
    "        plt.title(f\"Distribution of {feature}\", fontsize=10)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel(\"Density\", fontsize=12)\n",
    "        plt.xticks(fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.9])\n",
    "    plt.savefig(f'domain_distribution.png')\n",
    "\n",
    "def plot_corr(app_train, feature_list):\n",
    "    feature_list.append('TARGET')\n",
    "    ext_data = app_train[feature_list]\n",
    "    ext_data_corrs = ext_data.corr()\n",
    "    plt.figure(figsize = (14, 12))\n",
    "\n",
    "    # Heatmap of correlations\n",
    "    ax = sns.heatmap(ext_data_corrs, \n",
    "                     cmap = plt.cm.RdYlBu_r, \n",
    "                     vmin = -0.25, \n",
    "                     annot = True, \n",
    "                     vmax = 0.6,\n",
    "                     annot_kws={\"size\": 14}, \n",
    "                     cbar_kws={\"shrink\": 0.8, \"label\": {\"size\": 20}}\n",
    "    )\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=14, rotation=45, ha=\"right\") \n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=14, rotation=0)\n",
    "\n",
    "\n",
    "    plt.title(\"Correlation Heatmap\", fontsize=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'domain_correlation_heatmap.png')\n",
    "\n",
    "# Bin division drawing\n",
    "def plot_bin(app_train, feature_list):\n",
    "    for obj in feature_list:\n",
    "        age_data=app_train[['TARGET', obj]]\n",
    "\n",
    "        age_data[f'{obj}_BINNED']=pd.cut(age_data[obj], bins=np.linspace(age_data[obj].min(),age_data[obj].max(), num=11))\n",
    "        age_groups=age_data.groupby(f'{obj}_BINNED').mean()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        colors = sns.color_palette(\"husl\", n_colors=50)\n",
    "        plt.subplot(2,1 ,1)\n",
    "        plt.bar(age_groups.index.astype(str), \n",
    "                age_data[f'{obj}_BINNED'].value_counts(),\n",
    "                color=colors[0:len(age_groups.index.astype(str))],\n",
    "                width=0.4)\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=10) \n",
    "        plt.xlabel(f'{obj}_BINNED', fontsize=10) \n",
    "        plt.ylabel(\"Number of Features\", fontsize=10)\n",
    "\n",
    "        plt.subplot(2,1, 2)\n",
    "        plt.bar(age_groups.index.astype(str), \n",
    "                100*age_groups['TARGET'], \n",
    "                color=colors[0:len(age_groups.index.astype(str))],\n",
    "                width=0.4)\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "        plt.xlabel(f'{obj}_BINNED', fontsize=10)\n",
    "        plt.ylabel(\"Percent of Default rate [%]\", fontsize=10)\n",
    "\n",
    "        plt.suptitle(f'Distribution of {obj} and Default rate by TARGET', fontsize=13, y=0.98)\n",
    "\n",
    "        fig.align_ylabels()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{obj}_BINNED_user_profile.png')\n",
    "\n",
    "# model training for validation \n",
    "def model(features, test_features, cat_indices = [], n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "\n",
    "    object_list = ['NAME_EDUCATION_TYPE','NAME_HOUSING_TYPE','NAME_INCOME_TYPE','NAME_TYPE_SUITE']\n",
    "    cat_indices = []\n",
    "    for obj in object_list:\n",
    "         cat_indices.append(features.columns.get_loc(obj))\n",
    "\n",
    "    for index in cat_indices:\n",
    "         print(features.columns[index])\n",
    "         print(test_features.columns[index])\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50,\n",
    "                                   early_stopping_rounds = 100, verbose = 200,\n",
    "                                   verbosity= -1)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
